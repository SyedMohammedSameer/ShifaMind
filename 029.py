#!/usr/bin/env python3
"""
ShifaMind 029: Interactive Gradio Demo
Run this AFTER 028.py to launch interactive demo

REQUIREMENTS:
- Must run 028.py first (creates checkpoint and results)
- Loads: stage4_joint_best_revised.pt, reasoning_chains_50_samples.json, explainability_metrics.json

USAGE:
1. Run 028.py in Colab (full training)
2. Run this script: !python 029.py
3. Get public Gradio link

Mohammed Sameer Syed | University of Arizona | MS in AI
"""

# ============================================================================
# IMPORTS & SETUP
# ============================================================================
import os
import sys
import warnings
warnings.filterwarnings('ignore')

IN_COLAB = 'google.colab' in sys.modules
if IN_COLAB:
    print("üîß Installing Gradio...")
    os.system('pip install -q gradio')

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModel
from typing import Dict, List
from collections import defaultdict
import gradio as gr
import faiss

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üñ•Ô∏è  Device: {device}")

# ============================================================================
# DATA PATHS (same as 028.py)
# ============================================================================
BASE_PATH = Path('/content/drive/MyDrive/ShifaMind/01_Raw_Datasets/Extracted')
UMLS_PATH = BASE_PATH / 'umls-2025AA-metathesaurus-full/2025AA/META'
ICD_PATH = BASE_PATH / 'icd10cm-CodesDescriptions-2024'

# ============================================================================
# MINIMAL CLASSES (needed for model loading)
# ============================================================================

class ConceptStore:
    """Minimal concept store for demo"""
    def __init__(self, concepts):
        self.concepts = concepts

class EnhancedCrossAttention(nn.Module):
    """Cross-attention between text and concepts (EXACT copy from 028.py)"""
    def __init__(self, hidden_size, num_heads=8, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads

        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)

        self.gate = nn.Linear(hidden_size * 2, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_size)

    def forward(self, hidden_states, concept_embeddings, attention_mask=None):
        batch_size, seq_len, hidden_size = hidden_states.shape
        num_concepts = concept_embeddings.shape[0]

        concepts_batch = concept_embeddings.unsqueeze(0).expand(batch_size, -1, -1)

        Q = self.query(hidden_states).view(
            batch_size, seq_len, self.num_heads, self.head_dim
        ).transpose(1, 2)

        K = self.key(concepts_batch).view(
            batch_size, num_concepts, self.num_heads, self.head_dim
        ).transpose(1, 2)

        V = self.value(concepts_batch).view(
            batch_size, num_concepts, self.num_heads, self.head_dim
        ).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context = torch.matmul(attn_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, hidden_size
        )

        gate_input = torch.cat([hidden_states, context], dim=-1)
        gate_values = torch.sigmoid(self.gate(gate_input))
        output = hidden_states + gate_values * context

        output = self.layer_norm(output)

        return output, attn_weights.mean(dim=1)

class ShifaMindModel(nn.Module):
    """ShifaMind model (EXACT copy from 028.py)"""
    def __init__(self, base_model, concept_store, num_classes, fusion_layers=[9, 11]):
        super().__init__()
        self.base_model = base_model
        self.concept_store = concept_store
        self.num_classes = num_classes
        self.hidden_size = base_model.config.hidden_size
        self.fusion_layers = fusion_layers

        self.fusion_modules = nn.ModuleList([
            EnhancedCrossAttention(self.hidden_size, num_heads=8)
            for _ in fusion_layers
        ])

        self.diagnosis_head = nn.Linear(self.hidden_size, num_classes)
        self.concept_head = nn.Linear(self.hidden_size, len(concept_store.concepts))

        self.diagnosis_concept_interaction = nn.Bilinear(
            num_classes, len(concept_store.concepts), len(concept_store.concepts)
        )

        self.dropout = nn.Dropout(0.1)

    def forward(self, input_ids, attention_mask, concept_embeddings,
                return_diagnosis_only=False):
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        )

        if return_diagnosis_only:
            cls_hidden = outputs.last_hidden_state[:, 0, :]
            cls_hidden = self.dropout(cls_hidden)
            diagnosis_logits = self.diagnosis_head(cls_hidden)
            return {'logits': diagnosis_logits}

        hidden_states = outputs.hidden_states
        current_hidden = hidden_states[-1]

        fusion_attentions = []
        for i, fusion_module in enumerate(self.fusion_modules):
            layer_idx = self.fusion_layers[i]
            layer_hidden = hidden_states[layer_idx]

            fused_hidden, attn_weights = fusion_module(
                layer_hidden, concept_embeddings, attention_mask
            )
            fusion_attentions.append(attn_weights)

            if i == len(self.fusion_modules) - 1:
                current_hidden = fused_hidden

        cls_hidden = current_hidden[:, 0, :]
        cls_hidden = self.dropout(cls_hidden)

        diagnosis_logits = self.diagnosis_head(cls_hidden)
        concept_logits = self.concept_head(cls_hidden)

        diagnosis_probs = torch.sigmoid(diagnosis_logits)
        refined_concept_logits = self.diagnosis_concept_interaction(
            diagnosis_probs, torch.sigmoid(concept_logits)
        )

        return {
            'logits': diagnosis_logits,
            'concept_scores': refined_concept_logits,
            'attention_weights': fusion_attentions
        }

class DiagnosisAwareRAG:
    """Diagnosis-aware RAG - shares BERT model to save memory"""
    def __init__(self, icd_descriptions, tokenizer, device, bert_model=None):
        self.icd_descriptions = icd_descriptions
        self.tokenizer = tokenizer
        self.device = device
        self.bert_model = bert_model  # Shared BERT model
        self.documents = []
        self.doc_embeddings = None
        self.index = None

    def build_index(self):
        """Build FAISS index using shared BERT - processes in batches to avoid OOM"""
        # Create documents from ICD-10
        for code, description in self.icd_descriptions.items():
            self.documents.append({
                'text': f"ICD-10 {code}: {description}",
                'type': 'icd',
                'diagnosis_prefix': code[0]
            })

        # Encode documents in batches using shared BERT
        batch_size = 128  # Process 128 documents at a time (reduced to avoid OOM)
        texts = [doc['text'] for doc in self.documents]
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            inputs = self.tokenizer(batch_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')

            with torch.no_grad():
                # Use shared BERT model on GPU
                outputs = self.bert_model(**inputs.to(self.device))
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                all_embeddings.append(batch_embeddings)

            # Clear cache after each batch
            del inputs, outputs
            torch.cuda.empty_cache()

        # Combine all batches
        self.doc_embeddings = np.vstack(all_embeddings)

        # Normalize for cosine similarity
        norms = np.linalg.norm(self.doc_embeddings, axis=1, keepdims=True)
        self.doc_embeddings_normalized = self.doc_embeddings / (norms + 1e-10)

        # Build FAISS index
        dimension = self.doc_embeddings_normalized.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(self.doc_embeddings_normalized.astype('float32'))

    def retrieve(self, query_text, diagnosis_code=None, k=5):
        """Retrieve documents using shared BERT"""
        inputs = self.tokenizer([query_text], padding=True, truncation=True, max_length=512, return_tensors='pt')

        with torch.no_grad():
            # Use shared BERT model on GPU
            outputs = self.bert_model(**inputs.to(self.device))
            query_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()

        # Normalize query
        query_norm = np.linalg.norm(query_embedding, axis=1, keepdims=True)
        query_embedding_normalized = query_embedding / (query_norm + 1e-10)

        similarities, indices = self.index.search(query_embedding_normalized.astype('float32'), min(k * 3, len(self.documents)))

        results = []
        for sim, idx in zip(similarities[0], indices[0]):
            doc = self.documents[idx]
            if diagnosis_code and doc['diagnosis_prefix'] != diagnosis_code[0]:
                continue
            results.append({
                'document': doc['text'],
                'relevance': float(sim),
                'metadata': {'type': doc['type'], 'source': doc.get('diagnosis_prefix', '')}
            })
            if len(results) >= k:
                break

        return results

class ReasoningChainGenerator:
    """Generate reasoning chains (from 028.py)"""
    def __init__(self, model, tokenizer, concept_store, rag_system, target_codes, icd_descriptions, device):
        self.model = model
        self.tokenizer = tokenizer
        self.concept_store = concept_store
        self.rag_system = rag_system
        self.target_codes = target_codes
        self.icd_descriptions = icd_descriptions
        self.device = device

    def generate_reasoning_chain(self, clinical_text, concept_embeddings):
        """Generate complete reasoning chain for clinical text"""
        self.model.eval()

        inputs = self.tokenizer(
            clinical_text,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                concept_embeddings=concept_embeddings
            )

        diagnosis_probs = torch.sigmoid(outputs['logits'][0])
        concept_probs = torch.sigmoid(outputs['concept_scores'][0])

        top_diagnosis_idx = torch.argmax(diagnosis_probs).item()
        diagnosis_code = self.target_codes[top_diagnosis_idx]
        diagnosis_confidence = diagnosis_probs[top_diagnosis_idx].item()
        diagnosis_text = f"{diagnosis_code} - {self.icd_descriptions.get(diagnosis_code, 'Unknown')}"

        # DIAGNOSIS-SPECIFIC concept filtering (critical fix!)
        diagnosis_keywords = {
            'J189': ['pneumonia', 'lung', 'respiratory', 'infection', 'infiltrate', 'bacterial', 'aspiration'],
            'I5023': ['heart', 'cardiac', 'failure', 'cardiomyopathy', 'edema', 'ventricular', 'atrial'],
            'A419': ['sepsis', 'septicemia', 'bacteremia', 'infection', 'septic', 'shock', 'organ'],
            'K8000': ['cholecystitis', 'gallbladder', 'biliary', 'gallstone', 'cholelithiasis', 'bile']
        }

        # BLACKLIST wrong concepts per diagnosis (post-diagnosis filtering)
        CONCEPT_BLACKLIST = {
            'I5023': [
                'C0085740',  # Mendelson Syndrome (aspiration pneumonitis)
                'C0038166',  # Staphylococcal Skin Infections
                'C0276333',  # Parainfluenza virus pneumonia
                'C0152485',  # Other salmonella infections
                'C0275716',  # Infection due to other mycobacteria
            ],
            'J189': [],
            'A419': [],
            'K8000': []
        }

        # Filter concepts by diagnosis relevance
        concept_list = list(self.concept_store.concepts.items())
        relevant_keywords = diagnosis_keywords.get(diagnosis_code, [])
        blacklisted_cuis = CONCEPT_BLACKLIST.get(diagnosis_code, [])

        # Score and filter concepts
        scored_concepts = []
        for idx, (cui, concept_data) in enumerate(concept_list):
            score = concept_probs[idx].item()
            concept_name = concept_data['preferred_name'].lower()

            # Skip blacklisted concepts for this diagnosis
            if cui in blacklisted_cuis:
                continue

            # Check if concept matches diagnosis keywords
            is_relevant = any(kw in concept_name for kw in relevant_keywords)

            if is_relevant:
                # NO SCORE THRESHOLD - show all diagnosis-relevant concepts!
                scored_concepts.append({
                    'idx': idx,
                    'cui': cui,
                    'name': concept_data['preferred_name'],
                    'score': score
                })

        # Sort by score and take top 5
        scored_concepts = sorted(scored_concepts, key=lambda x: x['score'], reverse=True)[:5]

        if not scored_concepts:
            # Fallback: if ZERO relevant concepts found (shouldn't happen with protected concepts)
            # Show message instead of wrong concepts
            return {
                'diagnosis': diagnosis_text,
                'confidence': diagnosis_confidence,
                'reasoning_chain': [{
                    'claim': f'No {diagnosis_code}-relevant concepts found in concept store',
                    'concepts': [],
                    'evidence': [],
                    'attention_scores': []
                }],
                'rag_support': []
            }

        reasoning_chain = []
        for concept_info in scored_concepts:
            concept_idx = concept_info['idx']
            concept_cui = concept_info['cui']
            concept_data = {'preferred_name': concept_info['name']}
            concept_score = concept_info['score']

            # Extract evidence using attention
            # Use the last fusion layer's attention weights
            attn_weights = outputs['attention_weights'][-1]  # Shape: [batch, seq_len, num_concepts]
            concept_attn = attn_weights[:, :, concept_idx]  # Shape: [batch, seq_len]

            top_attn_indices = torch.argsort(concept_attn[0], descending=True)[:3]

            evidence_spans = []
            attention_scores = []
            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

            for attn_idx in top_attn_indices:
                start = max(0, attn_idx - 5)
                end = min(len(tokens), attn_idx + 6)
                span_tokens = tokens[start:end]
                span_text = self.tokenizer.convert_tokens_to_string(span_tokens)
                span_text = span_text.replace(' ##', '').replace('##', '').strip()

                if len(span_text) >= 20 and any(c.isalnum() for c in span_text):
                    evidence_spans.append(span_text)
                    attention_scores.append(concept_attn[0][attn_idx].item())

            if evidence_spans:
                # Create diagnosis-specific claims
                claim = self._create_claim(diagnosis_code, concept_data['preferred_name'])

                reasoning_chain.append({
                    'claim': claim,
                    'concepts': [{
                        'cui': concept_cui,
                        'name': concept_data['preferred_name'],
                        'score': concept_score
                    }],
                    'evidence': evidence_spans,
                    'attention_scores': attention_scores
                })

        # RAG support
        rag_docs = self.rag_system.retrieve(clinical_text, diagnosis_code, k=3)

        return {
            'diagnosis': diagnosis_text,
            'confidence': diagnosis_confidence,
            'reasoning_chain': reasoning_chain,
            'rag_support': rag_docs
        }

    def _create_claim(self, diagnosis_code, concept_name):
        """Create diagnosis-specific claim"""
        prefix = diagnosis_code[0]

        if prefix == 'J':  # Pneumonia
            if 'pneumonia' in concept_name.lower():
                return "Evidence of pneumonia or respiratory infection"
            return "Evidence of respiratory pathology"
        elif prefix == 'I':  # Heart failure
            if 'heart' in concept_name.lower() or 'cardiac' in concept_name.lower():
                return "Evidence of cardiac dysfunction"
            return "Evidence of cardiovascular pathology"
        elif prefix == 'A':  # Sepsis
            if 'sepsis' in concept_name.lower() or 'infection' in concept_name.lower():
                return "Evidence of systemic infection or sepsis"
            return "Evidence of infectious process"
        elif prefix == 'K':  # Cholecystitis
            if 'gallbladder' in concept_name.lower() or 'cholecyst' in concept_name.lower():
                return "Evidence of gallbladder disease"
            return "Evidence of biliary pathology"

        return f"Evidence supporting {diagnosis_code}"

# ============================================================================
# LOAD ICD-10 DESCRIPTIONS
# ============================================================================

def load_icd10_descriptions(icd_path):
    """Load ICD-10 descriptions"""
    descriptions = {}
    icd_file = icd_path / 'icd10cm-codes-2024.txt'

    with open(icd_file, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(maxsplit=1)
            if len(parts) == 2:
                code, desc = parts
                descriptions[code] = desc

    return descriptions

# ============================================================================
# LOAD UMLS (minimal, just for concept names)
# ============================================================================

def load_umls_concepts(checkpoint_concept_cuis):
    """Load UMLS concepts for CUIs in checkpoint - PRESERVES ORDER"""
    print("üìö Loading UMLS concepts...")

    # CRITICAL: Must preserve exact order from checkpoint!
    # Model indices depend on this order: index 0 ‚Üí checkpoint_concept_cuis[0], etc.
    from collections import OrderedDict

    mrconso_file = UMLS_PATH / 'MRCONSO.RRF'
    target_cuis_set = set(checkpoint_concept_cuis)

    # First pass: Load all target concepts from MRCONSO (unordered)
    temp_concepts = {}
    with open(mrconso_file, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            parts = line.strip().split('|')
            if len(parts) < 15:
                continue

            cui = parts[0]
            if cui not in target_cuis_set:
                continue

            lang = parts[1]
            if lang != 'ENG':
                continue

            preferred = parts[2] == 'P'
            name = parts[14]

            if cui not in temp_concepts or preferred:
                temp_concepts[cui] = {
                    'cui': cui,
                    'preferred_name': name,
                    'definition': ''
                }

    # Second pass: Rebuild in EXACT checkpoint order
    ordered_concepts = OrderedDict()
    for cui in checkpoint_concept_cuis:
        if cui in temp_concepts:
            ordered_concepts[cui] = temp_concepts[cui]
        else:
            # Fallback if CUI not found
            ordered_concepts[cui] = {
                'cui': cui,
                'preferred_name': f'Concept {cui}',
                'definition': ''
            }

    print(f"‚úÖ Loaded {len(ordered_concepts)} UMLS concepts")
    return ordered_concepts

# ============================================================================
# GRADIO DEMO FUNCTIONS
# ============================================================================

def format_diagnosis_output(chain):
    """Format diagnosis as HTML"""
    diagnosis = chain.get('diagnosis', 'Unknown')
    confidence = chain.get('confidence', 0.0)

    if confidence > 0.7:
        color = "#4caf50"
        label = "High Confidence"
    elif confidence > 0.5:
        color = "#ff9800"
        label = "Moderate Confidence"
    else:
        color = "#f44336"
        label = "Low Confidence"

    return f"""
    <div style="padding: 15px; background: white; border-left: 5px solid {color}; border-radius: 5px; margin: 10px 0;">
        <h3 style="margin: 0; color: {color};">{diagnosis}</h3>
        <div style="margin-top: 10px;"><strong>Confidence:</strong> {confidence:.3f} ({label})</div>
        <div style="background: #f5f5f5; border-radius: 5px; height: 20px; margin-top: 10px; overflow: hidden;">
            <div style="background: {color}; height: 100%; width: {confidence*100}%; transition: width 0.3s;"></div>
        </div>
    </div>
    """

def format_concepts_output(chain):
    """Format concepts as badges"""
    concepts = []
    for item in chain.get('reasoning_chain', [])[:5]:
        for concept in item.get('concepts', []):
            concepts.append(concept)

    if not concepts:
        return "<p>No concepts activated</p>"

    html = '<div style="display: flex; flex-wrap: wrap; gap: 10px; margin: 10px 0;">'
    colors = ['#2196f3', '#4caf50', '#ff9800', '#9c27b0', '#f44336']

    for i, concept in enumerate(concepts[:5]):
        color = colors[i % len(colors)]
        score = concept.get('score', 0.0)
        html += f"""
        <div style="background: {color}; color: white; padding: 8px 15px; border-radius: 20px; font-size: 0.9em;">
            <strong>{concept.get('name', 'Unknown')}</strong><br>
            <small>CUI: {concept.get('cui', 'N/A')} | Score: {score:.3f}</small>
        </div>
        """

    html += '</div>'
    return html

def format_evidence_output(clinical_text, chain):
    """Format evidence with highlighting"""
    all_evidence = []
    for item in chain.get('reasoning_chain', []):
        for evidence_text in item.get('evidence', []):
            all_evidence.append(evidence_text)

    if not all_evidence:
        return f'<div style="padding: 15px; background: #f5f5f5; border-radius: 5px;">{clinical_text[:500]}...</div>'

    highlighted = clinical_text[:500]
    colors = ['#ffeb3b', '#b3e5fc', '#c8e6c9', '#f8bbd0', '#d1c4e9']

    for i, evidence in enumerate(all_evidence[:5]):
        if evidence in highlighted:
            color = colors[i % len(colors)]
            highlighted = highlighted.replace(
                evidence,
                f'<mark style="background: {color}; padding: 2px 4px; border-radius: 3px;">{evidence}</mark>'
            )

    return f'<div style="padding: 15px; background: #f5f5f5; border-radius: 5px; line-height: 1.8; font-family: monospace; font-size: 0.85em;">{highlighted}...</div>'

def create_gradio_demo(model, tokenizer, concept_store, concept_embeddings, rag_system,
                      target_codes, icd_descriptions, demo_examples, metrics, device):
    """Create Gradio demo interface"""

    reasoning_generator = ReasoningChainGenerator(
        model, tokenizer, concept_store, rag_system,
        target_codes, icd_descriptions, device
    )

    # Prepare examples
    example_choices = []
    example_lookup = {}

    for i, ex in enumerate(demo_examples):
        diagnosis = ex.get('reasoning_chain', {}).get('diagnosis', 'Unknown')
        ground_truth = ', '.join(ex.get('ground_truth', []))
        label = f"Case {i+1}: {diagnosis[:50]}... (GT: {ground_truth})"
        example_choices.append(label)
        example_lookup[label] = ex

    def predict_with_explanation(clinical_text, example_name=None):
        if not clinical_text.strip():
            return "Please enter clinical text", "", "", ""

        if example_name and example_name in example_lookup:
            chain = example_lookup[example_name]['reasoning_chain']
        else:
            try:
                chain = reasoning_generator.generate_reasoning_chain(clinical_text, concept_embeddings)
            except Exception as e:
                return f"Error: {str(e)}", "", "", ""

        return (
            format_diagnosis_output(chain),
            format_concepts_output(chain),
            format_evidence_output(clinical_text, chain),
            json.dumps(chain, indent=2)
        )

    def load_example(example_name):
        if example_name in example_lookup:
            return example_lookup[example_name].get('clinical_text', '')[:500]
        return ""

    # Build interface
    with gr.Blocks(title="ShifaMind: Explainable Clinical AI", theme=gr.themes.Soft()) as demo:

        gr.Markdown(f"""
        # üè• ShifaMind: Enforced Explainability in Clinical AI

        **Capstone Project - Mohammed Sameer Syed**
        University of Arizona | MS in Artificial Intelligence

        ---

        ShifaMind achieves **{metrics.get('macro_f1', 0):.4f} F1 (+{metrics.get('improvement_pct', 0):.1f}% over baseline)** while providing complete
        reasoning chains: diagnosis ‚Üí concepts ‚Üí evidence ‚Üí knowledge base support.
        """)

        with gr.Tabs():
            # Tab 1: Clinical Diagnosis
            with gr.Tab("ü©∫ Clinical Diagnosis"):
                gr.Markdown("### Enter a clinical note or select an example")

                with gr.Row():
                    with gr.Column(scale=1):
                        example_selector = gr.Dropdown(
                            choices=example_choices,
                            label="Pre-loaded Examples",
                            value=example_choices[0] if example_choices else None
                        )
                        clinical_input = gr.Textbox(
                            label="Clinical Note",
                            placeholder="Enter discharge summary or clinical note...",
                            lines=8
                        )
                        predict_btn = gr.Button("üîç Analyze", variant="primary", size="lg")

                    with gr.Column(scale=2):
                        gr.Markdown("### Results")
                        diagnosis_output = gr.HTML()
                        concepts_output = gr.HTML()
                        evidence_output = gr.HTML()

                        with gr.Accordion("üîç Complete Reasoning Chain (JSON)", open=False):
                            reasoning_output = gr.Code(language="json", lines=15)

                example_selector.change(load_example, inputs=[example_selector], outputs=[clinical_input])
                predict_btn.click(
                    predict_with_explanation,
                    inputs=[clinical_input, example_selector],
                    outputs=[diagnosis_output, concepts_output, evidence_output, reasoning_output]
                )

            # Tab 2: Explainability Comparison
            with gr.Tab("üìä Explainability Comparison"):
                gr.Markdown("""
                ### Baseline vs. ShifaMind
                See the difference between a black-box model and ShifaMind's transparent reasoning.
                """)

                example_selector_2 = gr.Dropdown(
                    choices=example_choices,
                    label="Select Example",
                    value=example_choices[0] if example_choices else None
                )

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### ‚ùå Baseline Model")
                        baseline_output = gr.HTML()
                    with gr.Column():
                        gr.Markdown("#### ‚úÖ ShifaMind")
                        shifamind_output = gr.HTML()

                def show_comparison(example_name):
                    if example_name not in example_lookup:
                        return "No example selected", "No example selected"

                    chain = example_lookup[example_name].get('reasoning_chain', {})

                    baseline = f"""
                    <div style="padding: 20px; background: #f8f9fa; border-radius: 8px;">
                        <h3>{chain.get('diagnosis', 'Unknown')}</h3>
                        <p style="color: #666;">Confidence: {chain.get('confidence', 0.0):.3f}</p>
                        <p style="color: #999; font-style: italic;">No explanation available.</p>
                    </div>
                    """

                    concepts = chain.get('reasoning_chain', [])[:3]
                    concept_list = "<ul>"
                    for item in concepts:
                        concept_list += f"<li><strong>{item.get('claim', '')}</strong><ul>"
                        for concept in item.get('concepts', [])[:2]:
                            concept_list += f"<li>{concept.get('name', '')} (score: {concept.get('score', 0):.3f})</li>"
                        concept_list += "</ul></li>"
                    concept_list += "</ul>"

                    shifamind = f"""
                    <div style="padding: 20px; background: #e8f5e9; border-radius: 8px;">
                        <h3>{chain.get('diagnosis', 'Unknown')}</h3>
                        <p style="color: #666;">Confidence: {chain.get('confidence', 0.0):.3f}</p>
                        <h4>Reasoning:</h4>{concept_list}
                        <p style="color: #666; margin-top: 10px;">
                            ‚úÖ {len(chain.get('reasoning_chain', []))} concepts with evidence<br>
                            ‚úÖ {len(chain.get('rag_support', []))} supporting documents
                        </p>
                    </div>
                    """

                    return baseline, shifamind

                example_selector_2.change(show_comparison, inputs=[example_selector_2], outputs=[baseline_output, shifamind_output])

            # Tab 3: System Metrics
            with gr.Tab("üìà System Metrics"):
                gr.Markdown("### Performance & Explainability Metrics")

                with gr.Row():
                    with gr.Column():
                        gr.Markdown(f"""
                        #### Diagnostic Performance

                        | Metric | Baseline | ShifaMind | Improvement |
                        |--------|----------|-----------|-------------|
                        | **Macro F1** | {metrics.get('baseline_f1', 0):.4f} | **{metrics.get('macro_f1', 0):.4f}** | **+{metrics.get('improvement_pct', 0):.1f}%** |
                        | **Micro F1** | {metrics.get('baseline_micro_f1', 0):.4f} | {metrics.get('micro_f1', 0):.4f} | +{metrics.get('micro_improvement_pct', 0):.1f}% |
                        """)

                    with gr.Column():
                        gr.Markdown(f"""
                        #### Explainability Metrics

                        | Metric | Value |
                        |--------|-------|
                        | **Citation Completeness** | {metrics.get('citation_completeness', 0)*100:.1f}% |
                        | **Concept-Evidence Alignment** | {metrics.get('concept_evidence_alignment', 0)*100:.1f}% |
                        | **RAG Relevance** | {metrics.get('rag_relevance', 0)*100:.1f}% |
                        | **Avg Concepts/Diagnosis** | {metrics.get('avg_concepts_per_diagnosis', 0):.2f} |
                        """)

                if os.path.exists("shifamind_results.png"):
                    gr.Image(value="shifamind_results.png", label="Performance Comparison")

            # Tab 4: About
            with gr.Tab("‚ÑπÔ∏è About ShifaMind"):
                gr.Markdown("""
                ### System Overview

                **ShifaMind** enforces explainability through architectural design.

                #### Key Innovations:
                1. **Deep Ontology Integration** - UMLS concepts at transformer layers 9 & 11
                2. **Forced Citation Mechanism** - Every diagnosis has complete reasoning chain
                3. **Diagnosis-Conditional Labeling** - PMI-based concept selection

                #### Dataset:
                - **Source:** MIMIC-IV (8,604 discharge notes)
                - **Diagnoses:** J189 (Pneumonia), I5023 (Heart failure), A419 (Sepsis), K8000 (Cholecystitis)

                #### Model:
                - **Base:** Bio_ClinicalBERT (114.4M parameters)
                - **Training:** 4-stage pipeline (diagnosis ‚Üí concepts ‚Üí joint)

                #### Limitations:
                - Single institution data (MIMIC-IV)
                - Research prototype (not for clinical use)

                **Contact:** Mohammed Sameer Syed | University of Arizona | mohammedsameer@arizona.edu
                """)

    return demo

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*70)
    print("SHIFAMIND 029 - INTERACTIVE DEMO")
    print("="*70)

    # Check for required files
    required_files = [
        'stage4_joint_best_revised.pt',
        'reasoning_chains_50_samples.json',
        'explainability_metrics.json'
    ]

    missing_files = [f for f in required_files if not os.path.exists(f)]

    if missing_files:
        print("\n‚ùå ERROR: Missing required files from 028.py:")
        for f in missing_files:
            print(f"   - {f}")
        print("\n‚ö†Ô∏è  Please run 028.py first to generate these files!")
        sys.exit(1)

    print("\n‚úÖ Found all required files from 028.py")

    # Load checkpoint
    print("\nüîß Loading checkpoint...")
    checkpoint = torch.load('stage4_joint_best_revised.pt', map_location=device)
    concept_cuis = checkpoint['concept_cuis']
    num_concepts = checkpoint['num_concepts']

    # Load metrics
    print("üìä Loading metrics...")
    with open('explainability_metrics.json', 'r') as f:
        explainability_metrics = json.load(f)

    # Load reasoning chains
    print("üîó Loading reasoning chains...")
    with open('reasoning_chains_50_samples.json', 'r') as f:
        all_chains = json.load(f)

    # Calculate performance metrics from checkpoint
    baseline_f1 = 0.7150  # from 028.py typical baseline
    shifamind_f1 = checkpoint.get('f1_score', 0.7641)
    improvement = shifamind_f1 - baseline_f1
    improvement_pct = (improvement / baseline_f1) * 100

    metrics = {
        'baseline_f1': baseline_f1,
        'baseline_micro_f1': 0.7040,
        'macro_f1': shifamind_f1,
        'micro_f1': shifamind_f1,  # approximate
        'improvement_pct': improvement_pct,
        'micro_improvement_pct': improvement_pct,
        **explainability_metrics
    }

    print(f"   Macro F1: {shifamind_f1:.4f} (+{improvement_pct:.1f}%)")
    print(f"   Citation: {explainability_metrics.get('citation_completeness', 0)*100:.1f}%")
    print(f"   Alignment: {explainability_metrics.get('concept_evidence_alignment', 0)*100:.1f}%")
    print(f"   RAG: {explainability_metrics.get('rag_relevance', 0)*100:.1f}%")

    # Load ICD-10 descriptions
    print("\nüìö Loading ICD-10 descriptions...")
    icd10_descriptions = load_icd10_descriptions(ICD_PATH)

    # Load UMLS concepts
    umls_concepts = load_umls_concepts(concept_cuis)

    # Create concept store
    concept_store = ConceptStore(umls_concepts)

    # Target codes
    target_codes = ['J189', 'I5023', 'A419', 'K8000']

    # Initialize tokenizer
    print("\nüîß Initializing model components...")
    model_name = 'emilyalsentzer/Bio_ClinicalBERT'
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Create concept embeddings
    concept_texts = [
        f"{c['preferred_name']}: {c.get('definition', '')}"
        for c in umls_concepts.values()
    ]

    # Load BERT once and share it across all components
    print("   Loading shared BERT model...")
    bert_model = AutoModel.from_pretrained(model_name).to(device)

    # Create concept embeddings using shared BERT
    inputs = tokenizer(concept_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')
    with torch.no_grad():
        outputs = bert_model(**inputs.to(device))
        concept_embeddings = outputs.last_hidden_state[:, 0, :].clone()

    # Free temporary tensors (but keep bert_model)
    del outputs, inputs
    torch.cuda.empty_cache()

    print(f"   Concept embeddings: {concept_embeddings.shape}")

    # Initialize model using shared BERT (match 028.py signature)
    model = ShifaMindModel(
        base_model=bert_model,  # Reuse same BERT
        concept_store=concept_store,
        num_classes=len(target_codes),
        fusion_layers=[9, 11]
    )

    # Load weights
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    # Free checkpoint memory (huge - contains full model weights!)
    del checkpoint
    torch.cuda.empty_cache()
    print("   ‚úÖ Model loaded")

    # Build RAG system using shared BERT
    print("\nüîç Building RAG system...")
    rag = DiagnosisAwareRAG(icd10_descriptions, tokenizer, device, bert_model=bert_model)
    rag.build_index()
    print("   ‚úÖ RAG index built")

    # Select demo examples
    valid_chains = [c for c in all_chains if c.get('validation', {}).get('is_valid', False)]
    demo_examples = valid_chains[:10] if len(valid_chains) >= 10 else all_chains[:10]

    print(f"\n‚úÖ Prepared {len(demo_examples)} demo examples")

    # Create Gradio demo
    print("\nüé® Creating Gradio interface...")
    demo = create_gradio_demo(
        model=model,
        tokenizer=tokenizer,
        concept_store=concept_store,
        concept_embeddings=concept_embeddings,
        rag_system=rag,
        target_codes=target_codes,
        icd_descriptions=icd10_descriptions,
        demo_examples=demo_examples,
        metrics=metrics,
        device=device
    )

    print("\n" + "="*70)
    print("‚úÖ DEMO READY!")
    print("="*70)

    print("\nüìã System Status:")
    print(f"   ‚Ä¢ Loaded model from 028.py checkpoint")
    print(f"   ‚Ä¢ F1 Score: {shifamind_f1:.4f} (+{improvement_pct:.1f}%)")
    print(f"   ‚Ä¢ {len(demo_examples)} demo examples ready")
    print(f"   ‚Ä¢ {num_concepts} medical concepts")

    print("\nüöÄ Launching Gradio demo...")
    demo.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=None,  # Auto-find available port
        show_error=True
    )
