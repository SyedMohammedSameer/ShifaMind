# üè• ShifaMind Clinical Demo - Doctor Meeting

**Complete guide for running the ChatGPT vs ShifaMind comparison demo**

---

## üìã What You Need

Before the meeting, prepare:

1. **Google Colab** (free account)
2. **OpenAI API Key** - Get from https://platform.openai.com/api-keys
   - Make sure you have credits (~$0.50 should be enough)
3. **Ngrok Account** (free) - Get token from https://dashboard.ngrok.com/get-started/your-authtoken
4. **Model Checkpoint**: `stage4_joint_best_revised.pt` (from training)

---

## üöÄ Quick Start (5 Minutes)

### Step 1: Upload Files to Colab

1. Open Google Colab: https://colab.research.google.com/
2. Create new notebook
3. Upload these files to Colab:
   - `run_demo_colab.py`
   - `demo1.py`
   - `stage4_joint_best_revised.pt` (model checkpoint)

### Step 2: Run the Launcher

In a Colab cell, run:

```python
!python run_demo_colab.py
```

You'll be prompted to enter:
- **Ngrok auth token** (paste from https://dashboard.ngrok.com/get-started/your-authtoken)

Wait 10-15 seconds for setup.

### Step 3: Open the Demo

You'll see output like:
```
‚úÖ DEMO IS READY!
======================================================================
üåê Public URL: https://xxxx-xx-xxx-xxx-xxx.ngrok-free.app
```

**Click that URL** to open the demo in a new tab!

### Step 4: Configure in the Demo UI

In the Streamlit interface:

1. **Sidebar**: Enter your OpenAI API key
2. Click "**üöÄ Load ShifaMind Model**" (takes ~30 seconds)
3. Wait for "‚úÖ Model Ready" confirmation

### Step 5: Run a Comparison

1. Select a **template case** from dropdown (e.g., "Pneumonia Case")
2. Click "**üî¨ Run Diagnosis Comparison**"
3. Wait ~10-15 seconds
4. See side-by-side results!

---

## üéØ What the Demo Shows

### Left Side: ChatGPT-4
- ‚ùå Plain text response
- ‚ùå No structure
- ‚ùå No explainability
- ‚ùå Black box reasoning

### Right Side: ShifaMind
- ‚úÖ **Structured diagnoses** with ICD codes
- ‚úÖ **Confidence scores** for each diagnosis
- ‚úÖ **Medical concepts** that led to decision (UMLS-based)
- ‚úÖ **Concept definitions** for transparency
- ‚úÖ **Explainable** - can audit the reasoning

---

## üìä Template Cases Available

The demo includes 4 pre-written clinical cases:

1. **Pneumonia Case**
   - 67-year-old male with fever, cough, dyspnea
   - Shows respiratory concepts activation

2. **Heart Failure Case**
   - 72-year-old female with edema, orthopnea
   - Shows cardiac concepts (S3 gallop, JVD, etc.)

3. **Sepsis Case**
   - 81-year-old with confusion, hypotension, urosepsis
   - Shows infection-related concepts

4. **Cholecystitis Case**
   - 52-year-old with RUQ pain, Murphy's sign
   - Shows gallbladder-related concepts

You can also enter **custom clinical notes**!

---

## üéì Key Talking Points for the Doctor

### Why ShifaMind is Superior

1. **Explainability**
   - "Doctor, notice how ShifaMind shows *which medical concepts* led to the diagnosis"
   - "ChatGPT just gives text - you can't verify the reasoning"

2. **Clinical Grounding**
   - "ShifaMind uses UMLS ontology - the same concepts you use in practice"
   - "It's trained on MIMIC-IV real clinical notes"

3. **Structured Output**
   - "ICD-10 codes with confidence percentages"
   - "You can integrate this into EHR systems"

4. **Transparency**
   - "Every activated concept has a definition"
   - "You can audit why it made each decision"

5. **Trust**
   - "As a physician, you can verify the concepts make sense"
   - "ChatGPT is a black box - you can't validate its reasoning"

### Expected Questions & Answers

**Q: How accurate is it?**
> "We achieved 77.6% F1 score on MIMIC-IV test set. More importantly, it activates 17 clinically relevant concepts per case with 73% precision - these concepts align with what you'd expect for each diagnosis."

**Q: Can it handle rare diagnoses?**
> "Currently trained on 4 common conditions as proof-of-concept. The architecture is extensible to any ICD-10 codes - we just need labeled training data."

**Q: How does it compare to ChatGPT accuracy?**
> "ChatGPT may get the diagnosis right, but you can't verify *why*. ShifaMind shows the medical reasoning chain. For clinical decision support, explainability is as important as accuracy."

**Q: What about hallucinations?**
> "ShifaMind is constrained to UMLS concepts - it can't make up medical terms. ChatGPT can hallucinate conditions or reasoning that sounds plausible but is incorrect."

**Q: Can this replace doctors?**
> "Absolutely not! This is a *decision support tool*. It highlights relevant concepts for you to consider. You make the final clinical judgment. Think of it as an intelligent assistant that shows its work."

---

## üêõ Troubleshooting

### Model won't load
- Make sure `stage4_joint_best_revised.pt` is in the same directory
- Check Colab has GPU enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)
- Wait 30-60 seconds - model loading takes time

### ChatGPT error
- Verify API key is correct
- Check you have OpenAI credits: https://platform.openai.com/usage
- Try refreshing the page and re-entering the key

### Ngrok tunnel expired
- Free ngrok tunnels last 2 hours
- Just re-run `run_demo_colab.py` to create a new tunnel

### Demo is slow
- First run is slower (model loading)
- Subsequent runs are faster
- ChatGPT API calls take 3-5 seconds
- ShifaMind inference takes <1 second

### Concepts not showing
- This happens if model checkpoint is missing
- The demo will still work but show fewer concepts
- Make sure you uploaded the trained checkpoint

---

## üí° Demo Flow Recommendation

### For Maximum Impact (15 minutes)

1. **Introduction (2 min)**
   - "I want to show you the difference between general AI and specialized clinical AI"

2. **Run Pneumonia Case (3 min)**
   - Show ChatGPT response first
   - Then show ShifaMind
   - Point out concept activation

3. **Explain Concepts (3 min)**
   - Click on concept definitions
   - "Notice it found 'Pulmonary infiltrate', 'Rales', 'Dyspnea' - exactly what you'd document"
   - "ChatGPT can't show you this reasoning"

4. **Run Heart Failure Case (3 min)**
   - Show different concept activation
   - "See how it activated 'S3 gallop', 'JVD', 'Orthopnea'"
   - "These are the clinical signs you'd look for"

5. **Q&A and Discussion (4 min)**
   - Ask what they think
   - Discuss clinical utility
   - Get feedback on features they'd want

---

## üì∏ Screenshots to Capture

If you want to show screenshots later (without live demo):

1. **Comparison view** - Full screen showing both ChatGPT and ShifaMind
2. **Concept details** - Close-up of activated concepts with definitions
3. **Diagnosis confidence** - The confidence bars for each ICD code
4. **Multiple cases** - Side-by-side of different diagnosis types

---

## üéØ Success Metrics

Your demo is successful if the doctor:

‚úÖ Understands the explainability advantage
‚úÖ Can see the UMLS concepts make clinical sense
‚úÖ Appreciates the structured output format
‚úÖ Asks questions about extending to more diagnoses
‚úÖ Shows interest in clinical validation studies

---

## üìû Backup Plan

If technology fails:

1. **Have screenshots** ready on your phone/laptop
2. **Explain the concept** verbally using the template cases
3. **Show the code** structure (doctors respect rigor)
4. **Offer to send** a video recording later

---

## üôè Final Checklist

Before the meeting:

- [ ] Test the demo at least once
- [ ] Verify OpenAI API has credits
- [ ] Have ngrok token ready
- [ ] Upload model checkpoint
- [ ] Prepare 2-3 talking points
- [ ] Have backup screenshots
- [ ] Charge laptop fully
- [ ] Test internet connection

---

## üìù After the Meeting

Collect feedback:

1. What concepts would the doctor want to see?
2. What diagnoses would be most valuable?
3. How would they integrate this into workflow?
4. What accuracy threshold would they trust?
5. What additional features would make it clinically useful?

---

**Good luck with your demo! Insha'Allah it goes well! üöÄ**

*Bismillah - May Allah grant you success in your meeting.*

---

## üÜò Emergency Contact

If you have issues tomorrow morning, remember:

- Check the Colab output logs
- Verify all files uploaded correctly
- Try refreshing the ngrok URL
- Restart Colab runtime if needed
- Focus on explaining the CONCEPT even if demo fails

**The concept of explainable AI is what matters - the demo just helps visualize it!**
